# app.py
import os
import shutil
from typing import List, Optional
from fastapi import FastAPI, File, UploadFile, HTTPException, Depends, Form
from fastapi.middleware.cors import CORSMiddleware
from fastapi.responses import FileResponse
from pydantic import BaseModel
from sqlalchemy.orm import Session
from sqlalchemy import select, delete

from db.db import get_db
from db.models import Document, SearchLog
from loader import load_text
from indexer import rebuild_index, chroma

import numpy as np
from sklearn.decomposition import PCA
from sklearn.metrics.pairwise import cosine_similarity
from sentence_transformers import CrossEncoder

# ================================
# ì´ˆê¸° ì„¸íŒ…
# ================================
UPLOAD_DIR = "./data"
ALLOWED_EXT = {"txt", "pdf", "md", "docx", "pptx", "jpg", "jpeg", "png", "bmp", "tiff"}

app = FastAPI(title="FoundByMe API (Chroma + PostgreSQL)")

# ğŸš€ Re-ranker ëª¨ë¸ ë¡œë“œ (ì •í™•ë„ í–¥ìƒìš©)
# Cross-EncoderëŠ” ì†ë„ëŠ” ëŠë¦¬ì§€ë§Œ ì •í™•ë„ê°€ ë§¤ìš° ë†’ìŒ
print("[APP] Loading Re-ranker model...")
# ë‹¤êµ­ì–´ ì§€ì› ëª¨ë¸ ì‚¬ìš© (BAAI/bge-reranker-v2-m3: ì„±ëŠ¥ì´ ìš°ìˆ˜í•œ ë‹¤êµ­ì–´ ë¦¬ë­ì»¤)
reranker = CrossEncoder("BAAI/bge-reranker-v2-m3")
print("[APP] Re-ranker loaded.")

# CORS
app.add_middleware(
    CORSMiddleware,
    allow_origins=["*"],
    allow_methods=["*"],
    allow_headers=["*"],
)

class ChatRequest(BaseModel):
    query: str
    session_id: str = "default"

# ======================================================
# ğŸ” /search - ìœ ì‚¬ë¬¸ì„œ top5 + PCA 3D
# ======================================================
@app.get("/search")
def search(q: str, session_id: str = "default"):

    if not q:
        raise HTTPException(status_code=400, detail="query required")

    # session_id í•„í„° ì ìš©
    where_filter = {"session_id": session_id} if session_id != "default" else None
    
    # 1. 1ì°¨ ê²€ìƒ‰ (Vector Search) - í›„ë³´êµ°ì„ ë„‰ë„‰í•˜ê²Œ(15~20ê°œ) ê°€ì ¸ì˜´
    q_emb = chroma.embed([q])[0]
    candidate_k = 15
    result = chroma.collection.query(
        query_embeddings=[q_emb],
        n_results=candidate_k,
        where=where_filter
    )

    ids = result["ids"][0]
    docs = result["documents"][0] if result["documents"] else []
    metas = result["metadatas"][0] if result["metadatas"] else []
    
    real_k = len(ids)

    if real_k == 0:
        return {
            "query": q,
            "query_vector_3d": [0, 0, 0],
            "results": []
        }

    # 2. 2ì°¨ ê²€ìƒ‰ (Re-ranking) - CrossEncoderë¡œ ì •í™•ë„ ìˆœ ì •ë ¬
    # (ì§ˆë¬¸, ë¬¸ì„œë‚´ìš©) ìŒì„ ë§Œë“¤ì–´ ì ìˆ˜ ê³„ì‚°
    pairs = [[q, doc_text] for doc_text in docs]
    scores = reranker.predict(pairs)

    # ì ìˆ˜ì™€ ì¸ë±ìŠ¤ë¥¼ ë¬¶ì–´ì„œ ì •ë ¬ (ì ìˆ˜ ë†’ì€ ìˆœ)
    scored_results = []
    for i in range(real_k):
        scored_results.append({
            "index": i,
            "score": float(scores[i]),
            "id": ids[i],
            "doc": docs[i],
            "meta": metas[i]
        })
    
    # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
    scored_results.sort(key=lambda x: x["score"], reverse=True)

    # ìƒìœ„ 5ê°œë§Œ ì„ íƒ
    top_k = 5
    final_results = scored_results[:top_k]
    
    # 3D ì‹œê°í™”ë¥¼ ìœ„í•´ ì„ íƒëœ ë¬¸ì„œë“¤ì˜ Vectorë§Œ ë‹¤ì‹œ ê°€ì ¸ì˜¤ê¸° (ìµœì í™”)
    final_ids = [res["id"] for res in final_results]
    
    # Embedding ê°€ì ¸ì˜¤ê¸°
    doc_vecs = chroma.collection.get(ids=final_ids, include=["embeddings"])["embeddings"]
    query_vec = np.array(q_emb, dtype=np.float32)
    doc_vecs = np.array(doc_vecs, dtype=np.float32)

    # PCA
    if len(doc_vecs) > 0:
        X = np.vstack([query_vec, *doc_vecs])
    else:
        X = np.array([query_vec])
        
    if len(X) < 3:
         query_3d = [0, 0, 0]
         doc_3d = [[0,0,0]] * len(doc_vecs)
    else:
        pca = PCA(n_components=min(3, len(X)))
        X_3d = pca.fit_transform(X)
        if X_3d.shape[1] < 3:
            X_3d = np.pad(X_3d, ((0,0), (0, 3-X_3d.shape[1])), 'constant')
            
        query_3d = X_3d[0].tolist()
        doc_3d = X_3d[1:]
    
    results = []
    for i, res in enumerate(final_results):
        meta = res["meta"]
        results.append({
            "id": res["id"],
            "filename": meta.get("title"),
            "ext": meta.get("ext"),
            "page": meta.get("page", 1),
            "score": res["score"], 
            "vector_3d": doc_3d[i].tolist() if len(doc_3d) > i else [0,0,0],
            "preview": (res["doc"][:200] if res["doc"] else "").replace("\n", " "),
            "url": f"/api/files/{session_id}/{meta.get('title')}.{meta.get('ext')}"
        }) 
        
    # ì‚¬ìš©ì ìš”ì²­: query_3dì˜ ì²« ë²ˆì§¸ ê°’ì—ì„œ 3ì„ ëºŒ 
   
    
    return {
        "query": q,
        "query_vector_3d": query_3d,
        "results": results
    }

# =====================================
# ğŸ’¬ /chat (RAG Mock)
# =====================================
@app.post("/chat")
def chat_endpoint(req: ChatRequest, db: Session = Depends(get_db)):
    search_results = search(req.query, session_id=req.session_id)
    
    # Save query log
    try:
        # ì´ì „ ì§ˆë¬¸ ê¸°ë¡ ì‚­ì œ (í•œ ë²ˆì— í•˜ë‚˜ì˜ ì§ˆë¬¸ë§Œ ìœ ì§€)
        db.execute(delete(SearchLog).where(SearchLog.session_id == req.session_id))
        
        log = SearchLog(
            query=req.query, 
            session_id=req.session_id, 
            top_k=5, 
            results_count=len(search_results.get("results", []))
        )
        db.add(log)
        db.commit()
    except Exception as e:
        print(f"Error saving search log: {e}")
    
    sources = []
    context = ""
    
    if "results" in search_results:
        for res in search_results["results"]:
            sources.append({
                "name": res["filename"],
                "url": res.get("url", ""),
                "preview": res.get("preview", ""),
                "page": res.get("page", 1)
            })
            context += f"- {res['filename']} (p.{res.get('page', 1)})\n"
    
    if not context:
        answer = "ê´€ë ¨ëœ ë¬¸ì„œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ë¬¸ì„œë¥¼ ì—…ë¡œë“œí–ˆëŠ”ì§€ í™•ì¸í•´ì£¼ì„¸ìš”."
    else:
        answer = f"'{req.query}'ì— ëŒ€í•´ ë‹¤ìŒ ë¬¸ì„œë“¤ì„ ì°¾ì•˜ìŠµë‹ˆë‹¤:\n\n{context}"

    return {
        "query": req.query,
        "answer": answer,
        "sources": sources,
        "results": search_results.get("results", []),
        "query_vector_3d": search_results.get("query_vector_3d", [0,0,0])
    }


# ======================================================
# ğŸ“¤ /upload - íŒŒì¼ ì—…ë¡œë“œ
# ======================================================
@app.post("/upload")
def upload_file(
    files: List[UploadFile] = File(...),
    session_id: str = Form("default")
):
    saved_files = []
    errors = []

    session_dir = os.path.join(UPLOAD_DIR, session_id)
    os.makedirs(session_dir, exist_ok=True)

    for file in files:
        ext = file.filename.split(".")[-1].lower()
        if ext not in ALLOWED_EXT:
            errors.append(f"âŒ {file.filename}: ì§€ì›í•˜ì§€ ì•ŠëŠ” íŒŒì¼ í˜•ì‹ ({ext})")
            continue

        save_path = f"{session_dir}/{file.filename}"
        with open(save_path, "wb") as f:
            shutil.copyfileobj(file.file, f)
        
        saved_files.append(file.filename)

    return {
        "status": "completed",
        "saved": saved_files,
        "errors": errors,
        "session_id": session_id,
        "info": "ğŸ“Œ íŒŒì¼ ì €ì¥ì™„ë£Œ â†’ /reindex í˜¸ì¶œ ì‹œ ì¸ë±ì‹± ìˆ˜í–‰"
    }

# =====================================
# ğŸ“‚ íŒŒì¼ ë‹¤ìš´ë¡œë“œ/ì—´ê¸°
# =====================================
@app.get("/files/{session_id}/{filename}")
def get_file(session_id: str, filename: str):
    file_path = os.path.join(UPLOAD_DIR, session_id, filename)
    if os.path.exists(file_path):
        return FileResponse(file_path)
    return {"error": "File not found"}


# ======================================================
# ğŸ”„ /reindex - ì „ì²´ ì¬ì¸ë±ì‹±
# ======================================================
@app.get("/reindex")
def reindex(session_id: str = "default"):
    rebuild_index()
    # global chroma
    # chroma = ChromaEngine()  # reload - No need, using shared instance
    return {"status": "Success"}


# ======================================================
# ğŸ“„ /documents - ë¬¸ì„œ ëª©ë¡ ì¡°íšŒ
# ======================================================
@app.get("/documents")
def list_documents(session_id: str = "default", limit: int = 100, db: Session = Depends(get_db)):
    try:
        # SQL DBì—ì„œ íŒŒì¼ ëª©ë¡ ì¡°íšŒ (ì¤‘ë³µ ì—†ì´ íŒŒì¼ ë‹¨ìœ„ë¡œ)
        stmt = select(Document).where(
            (Document.path.like(f"%/{session_id}/%")) | 
            (Document.path.like(f"%\\{session_id}\\%"))
        )
        docs = db.execute(stmt).scalars().all()
        
        result = []
        for doc in docs:
            ext = doc.path.split(".")[-1]
            result.append({
                "id": str(doc.id),
                "filename": f"{doc.title}.{ext}", # í™•ì¥ì í¬í•¨
                "preview": doc.content[:50] if doc.content else "",
                "type": ext
            })
        return {"count": len(result), "documents": result}
    except Exception as e:
        print(f"Error listing documents: {e}")
        return {"documents": []}
        import traceback
        traceback.print_exc()
        raise HTTPException(status_code=500, detail=str(e))


# ======================================================
# ğŸ“Š /stats - í™•ì¥ìë³„ í†µê³„
# ======================================================
@app.get("/stats")
def stats():
    rows = chroma.collection.get(include=["metadatas"])
    metas = rows["metadatas"]

    stat = {}
    for meta in metas:
        ext = meta.get("ext", "txt")
        stat[ext] = stat.get(ext, 0) + 1

    return {
        "total_docs": len(metas),
        "by_extension": stat
    }

# =====================================
# ğŸŒŒ /galaxy (3D ì‹œê°í™”)
# =====================================
@app.get("/galaxy")
def galaxy_view(session_id: str = "default", query: Optional[str] = None, db: Session = Depends(get_db)):
    try:
        where_filter = {"session_id": session_id} if session_id != "default" else None
        
        results = chroma.collection.get(
            where=where_filter,
            include=["metadatas", "embeddings"]
        )
        
        ids = results["ids"]
        metas = results["metadatas"]
        embeddings = results["embeddings"]
        
        if not ids:
            return []

        vectors = []
        metadata_list = []
        
        for i, vec in enumerate(embeddings):
            vectors.append(vec)
            metadata_list.append({
                "id": ids[i],
                "label": metas[i].get("title", "unknown"),
                "type": metas[i].get("ext", "txt"),
                "page": metas[i].get("page", 1),
                "filename": metas[i].get("title", "unknown"),
                "isQuery": False
            })
            
        # Fetch all queries for this session from DB
        try:
            logs = db.execute(select(SearchLog).where(SearchLog.session_id == session_id)).scalars().all()
            queries = [log.query for log in logs]
            
            # Add current query if provided and not in logs
            if query and query not in queries:
                queries.append(query)
            
            # Deduplicate
            unique_queries = list(set(queries))
            
            if unique_queries:
                q_embeddings = chroma.embed(unique_queries)
                for q_text, q_vec in zip(unique_queries, q_embeddings):
                     vectors.append(q_vec)
                     metadata_list.append({
                        "id": f"query_{hash(q_text)}",
                        "label": f"Question: {q_text}",
                        "type": "query",
                        "isQuery": True
                    })
        except Exception as e:
            print(f"Error fetching queries: {e}")
            # Fallback to just the current query if DB fails
            if query:
                qvec = chroma.embed([query])[0]
                vectors.append(qvec)
                metadata_list.append({
                    "id": "query",
                    "label": f"Question: {query}",
                    "type": "query",
                    "isQuery": True
                })

        X = np.array(vectors)
        
        if len(X) < 3:
             points = []
             for i, meta in enumerate(metadata_list):
                points.append({
                    "id": meta["id"],
                    "position": [np.random.uniform(-5, 5), np.random.uniform(-5, 5), np.random.uniform(-5, 5)],
                    "color": "#FDE047" if meta["isQuery"] else "#8B5CF6",
                    "label": meta["label"],
                    "page": meta.get("page", 1),
                    "isQuery": meta["isQuery"]
                })
             return points

        # 1. PCA ìˆ˜í–‰ (ì „ì²´ ë°ì´í„°ì˜ êµ¬ì¡° íŒŒì•…)
        pca = PCA(n_components=3)
        X_3d = pca.fit_transform(X)
        
        # ì¤‘ì‹¬ì  ë§ì¶”ê¸° (í‰ê· ì„ 0ìœ¼ë¡œ)
        X_3d = X_3d - np.mean(X_3d, axis=0)
        
        # ìŠ¤ì¼€ì¼ë§ (í™”ë©´ì— ê½‰ ì°¨ê²Œ)
        max_val = np.max(np.abs(X_3d))
        if max_val > 0:
            X_3d = (X_3d / max_val) * 60

        points = []
        for i, coord in enumerate(X_3d):
            meta = metadata_list[i]
            
            color = "#8B5CF6"
            if meta["isQuery"]:
                color = "#FDE047"
            elif meta["type"] in ["pdf"]:
                color = "#F43F5E"
            elif meta["type"] in ["txt", "md"]:
                color = "#06B6D4"
            elif meta["type"] in ["pptx", "ppt"]:
                color = "#F97316"

            points.append({
                "id": meta["id"],
                "position": coord.tolist(),
                "color": color,
                "label": meta["label"],
                "page": meta.get("page", 1),
                "isQuery": meta["isQuery"],
                "url": f"/api/files/{session_id}/{meta.get('filename')}.{meta.get('type')}#page={meta.get('page', 1)}" if not meta["isQuery"] else None
            })
            
        return points

    except Exception as e:
        print(f"Galaxy View Error: {e}")
        return []

# ======================================================
# â‘¢ delete_session : ì±„íŒ…ë°© ì‚­ì œ
# ======================================================
@app.delete("/session/{session_id}")
def delete_session(session_id: str, db: Session = Depends(get_db)):
    print(f"Request to delete session: {session_id}")
    
    # 1. SQL DB ì‚­ì œ (Pythonì—ì„œ ê²½ë¡œ ê²€ì‚¬ë¡œ í™•ì‹¤í•˜ê²Œ ì²˜ë¦¬)
    try:
        # ëª¨ë“  ë¬¸ì„œ ì¡°íšŒ (IDì™€ Pathë§Œ)
        docs = db.execute(select(Document.id, Document.path)).all()
        ids_to_delete = []
        
        for doc_id, path in docs:
            # ê²½ë¡œ ì •ê·œí™” (ëª¨ë“  êµ¬ë¶„ìë¥¼ /ë¡œ ë³€ê²½)
            norm_path = path.replace("\\", "/")
            # session_idê°€ ê²½ë¡œì˜ ì¼ë¶€ì¸ì§€ í™•ì¸ (ì˜ˆ: data/session_id/file.pdf)
            parts = norm_path.split("/")
            if session_id in parts:
                ids_to_delete.append(doc_id)
        
        if ids_to_delete:
            db.execute(delete(Document).where(Document.id.in_(ids_to_delete)))
            db.commit()
            print(f"Deleted {len(ids_to_delete)} documents from SQL DB.")
        else:
            print("No documents found in SQL DB for this session.")
            
    except Exception as e:
        print(f"Error deleting from SQL: {e}")
        db.rollback()

    # 2. ChromaDB ì‚­ì œ
    try:
        chroma.collection.delete(where={"session_id": session_id})
        print(f"Deleted vectors for session {session_id} from ChromaDB.")
    except Exception as e:
        print(f"Error deleting from ChromaDB: {e}")

    # 3. ë¡œì»¬ íŒŒì¼ ì‚­ì œ
    session_dir = os.path.join(UPLOAD_DIR, session_id)
    if os.path.exists(session_dir):
        try:
            shutil.rmtree(session_dir)
            print(f"Deleted local directory: {session_dir}")
        except Exception as e:
            print(f"Error deleting local directory: {e}")
            # Windowsì—ì„œ íŒŒì¼ì´ ì‚¬ìš© ì¤‘ì¼ ê²½ìš° ì‹¤íŒ¨í•  ìˆ˜ ìˆìŒ
            return {"status": "PARTIAL_ERROR", "message": str(e)}
    else:
        print(f"Local directory not found: {session_dir}")

    return {
        "status": "DELETED",
        "session_id": session_id
    }

# ======================================================
# â‘£ delete_all_sessions : ëª¨ë“  ì±„íŒ…ë°© ì‚­ì œ
# ======================================================
@app.delete("/sessions")
def delete_all_sessions(db: Session = Depends(get_db)):
    print("Request to delete ALL sessions")
    
    # 1. SQL DB ì‚­ì œ (ëª¨ë“  ë¬¸ì„œ ë° ë¡œê·¸ ì‚­ì œ)
    try:
        db.execute(delete(Document))
        db.execute(delete(SearchLog))
        db.commit()
        print("Deleted all documents and logs from SQL DB.")
    except Exception as e:
        print(f"Error deleting all from SQL: {e}")
        db.rollback()

    # 2. ChromaDB ì‚­ì œ (ì „ì²´ ì‚­ì œ)
    try:
        # ëª¨ë“  ë°ì´í„° ì‚­ì œë¥¼ ìœ„í•´ get()ìœ¼ë¡œ IDë¥¼ ê°€ì ¸ì™€ì„œ ì‚­ì œí•˜ê±°ë‚˜ reset() ì‚¬ìš©
        # ì—¬ê¸°ì„œëŠ” collectionì˜ ëª¨ë“  ë°ì´í„°ë¥¼ ì‚­ì œ
        ids = chroma.collection.get()['ids']
        if ids:
            chroma.collection.delete(ids=ids)
        print("Deleted all vectors from ChromaDB.")
    except Exception as e:
        print(f"Error deleting all from ChromaDB: {e}")

    # 3. ë¡œì»¬ íŒŒì¼ ì‚­ì œ (data í´ë” ë‚´ì˜ ëª¨ë“  í•˜ìœ„ í´ë”/íŒŒì¼ ì‚­ì œ)
    if os.path.exists(UPLOAD_DIR):
        for item in os.listdir(UPLOAD_DIR):
            item_path = os.path.join(UPLOAD_DIR, item)
            try:
                if os.path.isdir(item_path):
                    shutil.rmtree(item_path)
                else:
                    os.remove(item_path)
            except Exception as e:
                print(f"Error deleting {item}: {e}")
        print("Deleted all local files.")

    return {"status": "ALL_DELETED"}

if __name__ == '__main__':
    import uvicorn
    uvicorn.run(app, host='0.0.0.0', port=8000)
